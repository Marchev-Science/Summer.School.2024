{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "NER_TAGS_COL = 'tags'\n",
    "\n",
    "raw_datasets = load_dataset(\"tner/mit_restaurant\")\n",
    "model_checkpoint = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "original_label_to_id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Rating\": 1,\n",
    "    \"I-Rating\": 2,\n",
    "    \"B-Amenity\": 3,\n",
    "    \"I-Amenity\": 4,\n",
    "    \"B-Location\": 5,\n",
    "    \"I-Location\": 6,\n",
    "    \"B-Restaurant_Name\": 7,\n",
    "    \"I-Restaurant_Name\": 8,\n",
    "    \"B-Price\": 9,\n",
    "    \"B-Hours\": 10,\n",
    "    \"I-Hours\": 11,\n",
    "    \"B-Dish\": 12,\n",
    "    \"I-Dish\": 13,\n",
    "    \"B-Cuisine\": 14,\n",
    "    \"I-Price\": 15,\n",
    "    \"I-Cuisine\": 16\n",
    "}\n",
    "original_id_to_label = {i:name for name, i in original_label_to_id.items()}\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Rating\": 1,\n",
    "    \"I-Rating\": 2,\n",
    "    \"B-Amenity\": 3,\n",
    "    \"I-Amenity\": 4,\n",
    "    \"B-Location\": 5,\n",
    "    \"I-Location\": 6,\n",
    "    \"B-Restaurant_Name\": 7,\n",
    "    \"I-Restaurant_Name\": 8,\n",
    "    \"B-Price\": 9,\n",
    "    \"I-Price\": 10,\n",
    "    \"B-Hours\": 11,\n",
    "    \"I-Hours\": 12,\n",
    "    \"B-Dish\": 13,\n",
    "    \"I-Dish\": 14,\n",
    "    \"B-Cuisine\": 15,\n",
    "    \"I-Cuisine\": 16\n",
    "}\n",
    "id2label = {i:name for name, i in label2id.items()}\n",
    "\n",
    "original_to_ours = {original_label_to_id[k]: label2id[k] for k in label2id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relable(examples, original_to_ours):\n",
    "    new_tags = []\n",
    "    for tags in examples[NER_TAGS_COL]:\n",
    "        new_tags.append([original_to_ours[item] for item in tags])\n",
    "    examples[NER_TAGS_COL] = new_tags\n",
    "    return examples\n",
    "\n",
    "raw_datasets = raw_datasets.map(relable, batched=True, fn_kwargs={'original_to_ours':original_to_ours})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 9, 15, 0, 5]\n",
      "[-100, 0, 0, 0, 0, 0, 9, 15, 16, 16, 0, 5, -100]\n"
     ]
    }
   ],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "labels = raw_datasets[\"train\"][0][NER_TAGS_COL]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can',\n",
       " 'you',\n",
       " 'find',\n",
       " 'me',\n",
       " 'the',\n",
       " 'cheapest',\n",
       " 'mexican',\n",
       " 'restaurant',\n",
       " 'nearby']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 64, 47, 465, 162, 5, 21084, 162, 1178, 12657, 2391, 3027, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ican'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('mexican')\n",
    "tokenizer.decode([12657])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now on the whole dataset\n",
    "def tokenize_and_align_labels(examples, is_split_into_words=True):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=is_split_into_words\n",
    "    )\n",
    "    all_labels = examples[NER_TAGS_COL]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets =raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0,\n",
       "   64,\n",
       "   47,\n",
       "   465,\n",
       "   162,\n",
       "   5,\n",
       "   21084,\n",
       "   162,\n",
       "   1178,\n",
       "   12657,\n",
       "   2391,\n",
       "   3027,\n",
       "   2],\n",
       "  [0, 64, 47, 465, 162, 5, 1367, 18079, 8453, 2],\n",
       "  [0, 64, 47, 465, 162, 5, 8099, 21629, 35427, 5566, 2],\n",
       "  [0, 64, 47, 465, 162, 5, 16198, 44355, 218, 5618, 29, 2],\n",
       "  [0, 64, 47, 465, 162, 5, 3673, 516, 20212, 3027, 19, 14591, 23, 5, 2003, 2]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       " 'labels': [[-100, 0, 0, 0, 0, 0, 9, 15, 16, 16, 0, 5, -100],\n",
       "  [-100, 0, 0, 0, 0, 0, 5, 7, 8, -100],\n",
       "  [-100, 0, 0, 0, 0, 0, 5, 7, 8, 8, -100],\n",
       "  [-100, 0, 0, 0, 0, 0, 5, 7, 8, 8, 8, -100],\n",
       "  [-100, 0, 0, 0, 0, 0, 7, 8, 8, 5, 0, 3, 4, 4, 4, -100]]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    9,   15,   16,   16,    0,    5,\n",
       "         -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    5,    7,    8, -100, -100, -100,\n",
       "         -100]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "def ner_metrics_factory(id2label: dict | list, module: str = \"seqeval\"):\n",
    "    \n",
    "    metric = evaluate.load(module)\n",
    "    def compute_metrics(eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "        # Remove ignored index (special tokens) and convert to labels\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return all_metrics\n",
    "    \n",
    "    return compute_metrics\n",
    "\n",
    "compute_metrics = ner_metrics_factory(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'B-Price', 'B-Cuisine', 'O', 'B-Location']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][NER_TAGS_COL]\n",
    "labels = [id2label[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\programs\\miniconda3\\envs\\prod\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Cuisine': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'Location': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'Price': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.8888888888888888}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[-1] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"roberta-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='no', #epoch\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"], #.select(range(100)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], #.select(range(100)),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/local_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained('model/local_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Rating',\n",
       "  'score': 0.8687941,\n",
       "  'word': ' best',\n",
       "  'start': 13,\n",
       "  'end': 17},\n",
       " {'entity_group': 'Cuisine',\n",
       "  'score': 0.47829947,\n",
       "  'word': ' american',\n",
       "  'start': 18,\n",
       "  'end': 26},\n",
       " {'entity_group': 'Dish',\n",
       "  'score': 0.40889955,\n",
       "  'word': ' style',\n",
       "  'start': 27,\n",
       "  'end': 32},\n",
       " {'entity_group': 'Dish',\n",
       "  'score': 0.54043555,\n",
       "  'word': ' steak',\n",
       "  'start': 33,\n",
       "  'end': 38}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"model/roberta-ner\"\n",
    "local_checkpoint = 'model/local_ner' # <--------------\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"average\"\n",
    ")\n",
    "token_classifier(\"Where is the best american style steak for breakfast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Rating',\n",
       "  'score': 0.9469606,\n",
       "  'word': ' best',\n",
       "  'start': 13,\n",
       "  'end': 17},\n",
       " {'entity_group': 'Dish',\n",
       "  'score': 0.67589,\n",
       "  'word': ' american style steak',\n",
       "  'start': 18,\n",
       "  'end': 38},\n",
       " {'entity_group': 'Hours',\n",
       "  'score': 0.8031102,\n",
       "  'word': ' breakfast',\n",
       "  'start': 43,\n",
       "  'end': 52}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"model/roberta-ner\"\n",
    "local_checkpoint = 'model/local_ner' # <--------------\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"average\"\n",
    ")\n",
    "token_classifier(\"Where is the best american style steak for breakfast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=tensor(2.8301, grad_fn=<NllLossBackward0>), logits=tensor([[[ 0.1519, -0.0436, -0.3144,  0.1548,  0.0801, -0.1030, -0.4586,\n",
       "          -0.1639,  0.0112, -0.2178,  0.0613, -0.0064,  0.3944, -0.2061,\n",
       "           0.1541,  0.3359, -0.0254],\n",
       "         [ 0.1326, -0.2831, -0.3997,  0.3890,  0.4758, -0.0217, -0.6431,\n",
       "          -0.1239, -0.2796, -0.2442, -0.2296,  0.3537,  0.4291, -0.2513,\n",
       "           0.2084,  0.5746,  0.2295],\n",
       "         [ 0.1017, -0.0653, -0.1833,  0.1308,  0.3437, -0.0944, -0.7408,\n",
       "          -0.2187, -0.3630, -0.3572, -0.2900,  0.2107,  0.6002, -0.0853,\n",
       "           0.2342,  0.3991,  0.2268],\n",
       "         [ 0.3121, -0.2272, -0.3717,  0.2202,  0.3417, -0.0545, -0.3707,\n",
       "          -0.3016, -0.4658, -0.3904, -0.3797,  0.3842,  0.4422, -0.0133,\n",
       "           0.1260,  0.6062,  0.4494],\n",
       "         [ 0.2335,  0.0402, -0.2908,  0.2044,  0.4027, -0.2161, -0.6017,\n",
       "          -0.2181, -0.1466, -0.2539, -0.1411,  0.3368,  0.5524, -0.3622,\n",
       "           0.2096,  0.4108,  0.1510],\n",
       "         [ 0.0863,  0.0216, -0.1497,  0.1844,  0.4538, -0.2624, -0.5966,\n",
       "          -0.2146, -0.5708, -0.5013, -0.3882,  0.2642,  0.4963,  0.0460,\n",
       "           0.1861,  0.5755,  0.2644],\n",
       "         [ 0.3605, -0.0947, -0.3495,  0.2716,  0.4644, -0.4530, -0.3462,\n",
       "          -0.1564, -0.5314, -0.5369, -0.3037,  0.3011,  0.5362,  0.3032,\n",
       "          -0.0240,  0.7060,  0.5662],\n",
       "         [ 0.2216,  0.2964, -0.2007,  0.1908,  0.1705, -0.2670, -0.4643,\n",
       "          -0.1619, -0.4948, -0.4006, -0.1142,  0.2044,  0.4313, -0.0130,\n",
       "           0.2387,  0.2409,  0.1015],\n",
       "         [ 0.2216,  0.1905, -0.1470,  0.1012,  0.1609,  0.0252, -0.0805,\n",
       "          -0.0575, -0.2681, -0.3964, -0.2502,  0.3973,  0.3592, -0.0040,\n",
       "           0.2015,  0.4674,  0.1333],\n",
       "         [ 0.0855,  0.1381, -0.2464,  0.1811,  0.2194, -0.2506, -0.1582,\n",
       "           0.0187, -0.3427, -0.2300, -0.2194,  0.4879,  0.4582, -0.0337,\n",
       "           0.2320,  0.4707,  0.4497],\n",
       "         [ 0.1414,  0.0649, -0.2400,  0.0833,  0.6141, -0.2948, -0.3170,\n",
       "           0.0712, -0.5783, -0.4627, -0.1930,  0.3205,  0.5413,  0.0728,\n",
       "           0.0551,  0.5964,  0.5093],\n",
       "         [ 0.1910,  0.0574, -0.2641,  0.2222,  0.4322, -0.2016, -0.3955,\n",
       "          -0.2321, -0.3685, -0.4715, -0.3248,  0.3930,  0.6692,  0.0989,\n",
       "           0.1939,  0.4948,  0.4213],\n",
       "         [ 0.1481, -0.0584, -0.3332,  0.1508,  0.0875, -0.0931, -0.4718,\n",
       "          -0.1927, -0.0012, -0.1985,  0.0430,  0.0160,  0.4180, -0.2147,\n",
       "           0.1518,  0.3309, -0.0261]],\n",
       "\n",
       "        [[ 0.1694, -0.0478, -0.3335,  0.1466,  0.0795, -0.1016, -0.4567,\n",
       "          -0.1831, -0.0123, -0.2365,  0.0482, -0.0116,  0.4095, -0.1990,\n",
       "           0.1567,  0.3192, -0.0444],\n",
       "         [ 0.1771, -0.2359, -0.4988,  0.4433,  0.3922, -0.1117, -0.6278,\n",
       "          -0.1071, -0.3337, -0.3148, -0.2673,  0.2148,  0.5427, -0.3152,\n",
       "           0.2133,  0.5553,  0.1206],\n",
       "         [ 0.1722, -0.1113, -0.2874,  0.1413,  0.3109, -0.1898, -0.7291,\n",
       "          -0.2309, -0.4039, -0.3722, -0.3289,  0.2035,  0.6966, -0.0986,\n",
       "           0.3390,  0.2994,  0.2038],\n",
       "         [ 0.3390, -0.1089, -0.4998,  0.2414,  0.2941, -0.0232, -0.4226,\n",
       "          -0.3324, -0.3740, -0.4105, -0.3821,  0.3765,  0.5748, -0.0922,\n",
       "           0.1378,  0.4024,  0.3490],\n",
       "         [ 0.2454,  0.0039, -0.2993,  0.1371,  0.2354, -0.2938, -0.6210,\n",
       "          -0.1435, -0.1049, -0.4303, -0.0609,  0.4657,  0.6062, -0.2895,\n",
       "           0.2339,  0.2535,  0.0980],\n",
       "         [ 0.1095,  0.0289, -0.3423,  0.2131,  0.2359, -0.2288, -0.5531,\n",
       "          -0.1873, -0.3375, -0.4531, -0.1357,  0.2484,  0.6517, -0.1107,\n",
       "           0.2929,  0.2393,  0.1775],\n",
       "         [ 0.3469,  0.1497, -0.3989,  0.3147,  0.2060, -0.2438, -0.5845,\n",
       "          -0.2908, -0.3386, -0.2986, -0.1592,  0.2908,  0.7653, -0.1621,\n",
       "           0.1760,  0.3368,  0.2739],\n",
       "         [ 0.2628,  0.1427, -0.3927,  0.2486,  0.3173, -0.0924, -0.3377,\n",
       "          -0.1389, -0.3688, -0.2818, -0.2519,  0.3957,  0.6310,  0.0340,\n",
       "           0.1535,  0.3665,  0.3045],\n",
       "         [ 0.1935,  0.0167, -0.4394,  0.1276,  0.2319, -0.1718, -0.6544,\n",
       "          -0.1671, -0.2725, -0.5168, -0.1087,  0.3211,  0.6821, -0.1664,\n",
       "           0.2673,  0.4970,  0.2028],\n",
       "         [ 0.1705, -0.0660, -0.3611,  0.1400,  0.0873, -0.0919, -0.4740,\n",
       "          -0.2152, -0.0310, -0.2265,  0.0265,  0.0041,  0.4409, -0.2060,\n",
       "           0.1592,  0.3082, -0.0486],\n",
       "         [ 0.0791,  0.0143, -0.3692,  0.2290,  0.2683, -0.1211, -0.5618,\n",
       "          -0.2442, -0.2049, -0.2609, -0.1755,  0.0676,  0.5847, -0.2081,\n",
       "           0.2383,  0.3979,  0.2434],\n",
       "         [ 0.0791,  0.0143, -0.3692,  0.2290,  0.2683, -0.1211, -0.5618,\n",
       "          -0.2442, -0.2049, -0.2609, -0.1755,  0.0676,  0.5847, -0.2081,\n",
       "           0.2383,  0.3979,  0.2434],\n",
       "         [ 0.0791,  0.0143, -0.3692,  0.2290,  0.2683, -0.1211, -0.5618,\n",
       "          -0.2442, -0.2049, -0.2609, -0.1755,  0.0676,  0.5847, -0.2081,\n",
       "           0.2383,  0.3979,  0.2434]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**data_collator([tokenized_datasets[\"train\"][i] for i in range(2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['can',\n",
       "  'you',\n",
       "  'locate',\n",
       "  'a',\n",
       "  'place',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'that',\n",
       "  'has',\n",
       "  'steak',\n",
       "  'on',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'and',\n",
       "  'serves',\n",
       "  'breakfast',\n",
       "  'all',\n",
       "  'day'],\n",
       " 'tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 15, 11, 12]}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'Dish',\n",
       "  'score': 0.924998,\n",
       "  'word': ' steak',\n",
       "  'start': 39,\n",
       "  'end': 44},\n",
       " {'entity_group': 'Hours',\n",
       "  'score': 0.72994566,\n",
       "  'word': ' breakfast all day',\n",
       "  'start': 68,\n",
       "  'end': 85}]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"model/roberta-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"average\"\n",
    ")\n",
    "token_classifier(\"Can you locate a place to eat that has steak on the menu and serves breakfast all day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.classifier.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(**data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 17])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
