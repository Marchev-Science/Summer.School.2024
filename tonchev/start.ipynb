{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "NER_TAGS_COL = 'tags'\n",
    "\n",
    "raw_datasets = load_dataset(\"tner/mit_restaurant\")\n",
    "model_checkpoint = \"FacebookAI/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "original_label_to_id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Rating\": 1,\n",
    "    \"I-Rating\": 2,\n",
    "    \"B-Amenity\": 3,\n",
    "    \"I-Amenity\": 4,\n",
    "    \"B-Location\": 5,\n",
    "    \"I-Location\": 6,\n",
    "    \"B-Restaurant_Name\": 7,\n",
    "    \"I-Restaurant_Name\": 8,\n",
    "    \"B-Price\": 9,\n",
    "    \"B-Hours\": 10,\n",
    "    \"I-Hours\": 11,\n",
    "    \"B-Dish\": 12,\n",
    "    \"I-Dish\": 13,\n",
    "    \"B-Cuisine\": 14,\n",
    "    \"I-Price\": 15,\n",
    "    \"I-Cuisine\": 16\n",
    "}\n",
    "original_id_to_label = {i:name for name, i in original_label_to_id.items()}\n",
    "\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-Rating\": 1,\n",
    "    \"I-Rating\": 2,\n",
    "    \"B-Amenity\": 3,\n",
    "    \"I-Amenity\": 4,\n",
    "    \"B-Location\": 5,\n",
    "    \"I-Location\": 6,\n",
    "    \"B-Restaurant_Name\": 7,\n",
    "    \"I-Restaurant_Name\": 8,\n",
    "    \"B-Price\": 9,\n",
    "    \"I-Price\": 10,\n",
    "    \"B-Hours\": 11,\n",
    "    \"I-Hours\": 12,\n",
    "    \"B-Dish\": 13,\n",
    "    \"I-Dish\": 14,\n",
    "    \"B-Cuisine\": 15,\n",
    "    \"I-Cuisine\": 16\n",
    "}\n",
    "id2label = {i:name for name, i in label2id.items()}\n",
    "\n",
    "original_to_ours = {original_label_to_id[k]: label2id[k] for k in label2id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relable(examples, original_to_ours):\n",
    "    new_tags = []\n",
    "    for tags in examples[NER_TAGS_COL]:\n",
    "        new_tags.append([original_to_ours[item] for item in tags])\n",
    "    examples[NER_TAGS_COL] = new_tags\n",
    "    return examples\n",
    "\n",
    "raw_datasets=raw_datasets.map(relable, batched=True, fn_kwargs={'original_to_ours':original_to_ours})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "labels = raw_datasets[\"train\"][0][NER_TAGS_COL]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now on the whole dataset\n",
    "def tokenize_and_align_labels(examples, is_split_into_words=True):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=is_split_into_words\n",
    "    )\n",
    "    all_labels = examples[NER_TAGS_COL]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets =raw_datasets.map(tokenize_and_align_labels, batched=True, remove_columns=raw_datasets['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "def ner_metrics_factory(id2label: dict | list, module: str = \"seqeval\"):\n",
    "    \n",
    "    metric = evaluate.load(module)\n",
    "    def compute_metrics(eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "        # Remove ignored index (special tokens) and convert to labels\n",
    "        true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return all_metrics\n",
    "    \n",
    "    return compute_metrics\n",
    "\n",
    "compute_metrics = ner_metrics_factory(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = raw_datasets[\"train\"][0][NER_TAGS_COL]\n",
    "labels = [id2label[i] for i in labels]\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"roberta-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='no', #epoch\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].select(range(100)),\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(100)),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"model/roberta-ner\"\n",
    "local_checkpoint = 'model/local_ner' # <--------------\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=local_checkpoint, aggregation_strategy=\"average\"\n",
    ")\n",
    "token_classifier(\"Can you locate a place to eat that has steak on the menu and serves breakfast all day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"model/roberta-ner\"\n",
    "local_checkpoint = 'model/local_ner' # <--------------\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=local_checkpoint, aggregation_strategy=\"average\"\n",
    ")\n",
    "token_classifier(\"Can you locate a place to eat that has steak on the menu and serves breakfast all day\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
